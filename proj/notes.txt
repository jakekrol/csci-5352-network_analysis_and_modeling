

04/01/2025
- debugged rank2g.py's rank2g function:
    - needed to pass in rank table, not correlation matrix
experiment 1 (done)
steps:
    - compute log return for each t (done)
    - compute correlation matrix (done)
    - compute correlation distribution (done)
    - get mean and std of the distribution (done)
    - run cor2rank for all corr matrices (done)
    - run rank2g.py with m = 464 choose 2 = n choose 2 = 107416, plot at 10, 50, 100, 1000 nodes
        - let's do m = 1000, for now. n choose 2 is too large
        - computing the stats takes too long

idea: compute assortativity of corr and random graphs

next steps:
    1. experiment 2: make corr graphs at t=1 and let m vary again
    - make a correlation matrix for each year separately
    - let t = 1, 5, 20, 60, 240, and m vary up to 1000
    - compute the same stats as before
    steps:
        - split prices-split-adjusted by year
        - run same pipeline as experiment 1
    2. Analysis of experiment 1 results

03/30/2025
do log return for t=5, 20, 60, 240
steps:
    - (done) modify price2cor.py to support diff t
        - see price2return.py
results:
    - ran price2return.py for t=5, 20, 60, 240
        - outputs: 
            - log return matrix
            - correlation matrix
            - correlation distribution

- ran cor2rank for all corr matrices
- running rank2.gpy for all ranks
prev cmds: ./rank2g.py -i topk.csv -r -m 100 -s 10,50,100 -p -o rand
maybe do 10,000 edges total (m)
and steps 10, 50, 100, 1000

next steps:
    -debug rank2g's rank2g function, very slow
    probably the unused combination  computation

03/29/2025
The graph I generated on 03/27/2025 is clusters very well by sector 
- # of edges: ~100
- \deltat = 1
- added node metadata

do random graph generation
done: see rank2graph.py's cor2g_rand function

experiment 1:
    T = 1762
    try the following ts: 1 (daily), 5 (weekly), 20 (monthly), 60 (quarterly), 240 (annual)
    where each trading week has 5 days

    how to choose max m?
        for t=1, the correlation distribution is zero-inflated, can go until corr is less than 0.1
    ? what does corr dist look like for other t?
    or use 1 standard deviation above the mean of correlation distribution
    steps:
        - compute log return for each t
        - compute correlation matrix
        - compute correlation distribution
        - get mean and std of the distribution
        - choose m = mean + std
        - rank 

next steps:
    - do log return for t=5, 20, 60, 240
    - compute correlation matrix
    - compute correlation distribution
    - finish experiment 1

03/27/2025
experiment 1:
let the t and m vary and measure (clustering coefficient [C], LCC size, # of components, mean degree, diameter, etc.)
steps:
1. see how I get corr matrix
    - price2cor.py
        - finds symbols with overlapping dates
        - computes a log return vector for t step (t=1) for now
        - computes correlation of the log return vectors
    - what are next steps?
    - write script to generate network from corr matrix
        - rank the correlations 
            - can use min heap to keep track of top k correlations
                - ex here: https://github.com/jakekrol/genefusion/blob/main/genefusion/genefusion/genefusion.py#L13-L30
                - maybe store as json {r1: {s_i: <symbol i>, s_j: <symbol j>, w: <corr>} ...}
    
problem: seems like some symbols are duplicated based 
on whether shareholders have voting rights or not
- let's choose the ones without voting rights
    - excluded symbols: GOOGL, FOXA, DISCA

idea: at each step track if the edge was drawn for
companies in the same sector

idea: plot the corr distribution for a given t
    - looks normally distributed for t=1

idea: plot the graph at m = 10, 1000, 10000

2. write script to compute G stats at various m
    - stats:
        - clustering coefficient [C]
        - LCC size
        - # of components
        - mean degree
        - diameter
3. next write a script for random graph

